{
    "metadata": {
        "language_info": {
            "file_extension": ".py", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython2", 
            "version": "2.7.11", 
            "name": "python", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6", 
            "language": "python", 
            "name": "python2"
        }
    }, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<h1>Supervised Learning in a Nutshell</h1>"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/SupervisedLearning.png?raw=true)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "<h1>Supervised Classification</h1> \n<h2>\n<ul>\n<li>\nDuring training, a feature extraction scheme is used to convert each input value to a feature vectors. <br> \nLabeled points, which consist of pairs of labels and feature vectors are fed into the machine learning <br>\nalgorithm to generate a model.\n</li>\n<p>\n<li>\nDuring prediction, the same feature extraction scheme is used to convert unseen inputs to feature vectors. <br>\nThese feature sets are then fed into the model, which generates predicted labels.\n</li>\n</ul>\n</h2>\n\n![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/Supervised-classification.png?raw=true)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<h1>Organization of the dataset for training supervised classifiers</h1> \n<h2>\n<ul>\n<li>The dataset is divided into two sets: (1) the development set, and (2) the \"test\" set.</li>\n<p>\n<li>The development set can further be subdivided into a \"training\" set and a \"dev\"-test set.</li>\n</ul>\n</h2>\n\n![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/Corpus-org.png?raw=true)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 3, 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 5, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 6, 
            "source": "rdd = sc.textFile(path_1)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<h1>ML Pipeline</h1>\n<h2>In machine learning, it is common to run a sequence of algorithms to process and learn from data. <br>\nE.g., a simple text document processing workflow might include several stages:</h2>\n<h2><ol>\n<li>Split each document\u2019s text into words.</li>\n<li>Convert each document\u2019s words into a numerical feature vector.</li>\n<li>Learn a prediction model using the feature vectors and labels.</li>\n</ol></h2>\n<h2>Spark ML represents such a workflow as a <u>Pipeline</u>, which consists of a sequence of PipelineStages <br>\n(Transformers and Estimators) to be run in a specific order.</h2> "
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<h1>Pipeline components</h1>\n<h2>Transformers</h2>\n<h3>\n<ol>\n<li>A Transformer is an abstraction that includes feature transformers and learned models.\n</li>\n<p>\n<li>\nTechnically, a Transformer implements a method transform(), which converts one DataFrame <br>\ninto another, generally by appending one or more columns.\n</li>\n<p>For example: \n<ul>\n<li>A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new <br>\ncolumn (e.g., feature vectors), and <u>output a new DataFrame with the mapped column appended</u>.<br>\n</li>\n<p>\n<li>A learning model might take a DataFrame, read the column containing feature vectors, predict <br> \nthe label for each feature vector, and <u>output a new DataFrame with predicted labels appended <br> \nas a column.</u>\n</li>\n</ul>\n</ol>\n</h3>\n\n<h2>Estimators</h2>\n<h3>\n<ol>\n<li>An Estimator abstracts the concept of a learning algorithm or any algorithm that fits \nor trains on data. \n</li>\n<p>\n<li>\nTechnically, <u>an Estimator implements a method fit(), which accepts a DataFrame and produces <br>\na Model, which is a Transformer</u>. <br> \n<p>For example:\n<p>\nA learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a <br>\nLogisticRegressionModel, which is a Model and hence a Transformer.\n</li>\n</ol>\n![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/ml-Pipeline.png?raw=true)\n<hr size=\"5\">\n![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/ml-PipelineModel.png?raw=true)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 7, 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "interim = rdd.map(lambda line: (1.0 if line.split(\"\\t\")[0] == \"spam\" else 0.0, line.split(\"\\t\")[1]))\ninterim = interim.map(lambda t: Row(label=t[0], text=t[1]))\ntraining = sqlContext.createDataFrame(interim)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "# Configure an ML pipeline, which consists of 3 stages: \n# (1) Tokenizer, \n#\u00a0(2) HashingTF, and \n#\u00a0(3) LR.\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\n# Fit the pipeline to training documents.\nmodel = pipeline.fit(training)\n\n# Prepare test documents, which are unlabeled (id, text) tuples.\ntest = sqlContext.createDataFrame([\n    Row(text=\"You will get a prize. To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"),\n    Row(text=\"Even my brother is not like to speak with me. They treat me like aids patent.\")])\n\n# Make predictions on test documents and print columns of interest.\nprediction = model.transform(test)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "prediction.limit(2)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": "selected = prediction.select(\"text\", \"prediction\")\nfor row in selected.collect():\n    print(row)"
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 0
}